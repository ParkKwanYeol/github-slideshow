Hold out validation

import numpy as np

num_validation_samples=10000

np.random.shuffle(data)

validation_data=data[:num_validation_samples]
data=data[num_validation_samples:]

training_data=data[:]

model=get_model()
model.train(training_data)
validation_score=model.evaluate(validation_data)

model=get_model()
model.train(np.concatenate([training_data,validation_data]))
test.score=model_evaluate(test_data)

K-fold cross-validation
k=4 #몇개의 fold
num_validation_samples=len(data) // k #validation sample에 몇개를 쓴지
np.random.shuffle(data) #data random shuffling
valdation_scores=[] #validation score를 4번에 걸쳐 구하면서 여기에 지정
for fold in range(k): #for문으로 fold를 4개를 돌리면서, training data는 validation set말고 다른 3개를 합쳐서 한다.
    validation_data=data[num_validation_samples * fold: num_validation_samples * (fold+1)]
    training_data=data[:num_validation_samples * fold] + data[num_validation_samples * (fold+1):]
    
    model=get_model()
    model.train(training_data)
    validation_score=model.evaluate(validation_data)
    validation_scores.append(validation_score)

validation_score=np.average(validation_scores) #최종 score는 validation_scores에 있는 값들을 평균내서 한다.

model=get_model()
model.train(data)
test_score=model.evaluate(test_data)

Reducing the network's size

Original model

from keras import models
from keras import layers

original_model=model.Sequential()
original_model.add(layers.Dense(16,activation='relu',input_shape=(10000,)))
original_model.add(layers.Dense(16,activation='relu'))
original_model.add(layers.Dense(1,activation='sigmoid'))

original_model.compile(optimizer='rmsprop',
                       loss='binary_crossentropy',
                       metrics=['acc'])

original network train

original_hist=original_model.fit(x_train,y_train,
                                 epochs=20,
                                 batch_size=512,
                                 validation_data=(x_test,y_test))
								 
Version of the model with lower capacity¶

smaller_model=model.Sequential()
smaller_model.add(layers.Dense(4,activation='relu',input_shape=(10000,)))
smaller_model.add(layers.Dense(4,activation='relu'))
smaller_model.add(layers.Dense(1,activation='sigmoid'))

smaller_model.compile(optimizer='rmsprop',
                       loss='binary_crossentropy',
                       metrics=['acc'])

smaller network train

smaller_hist=smaller_model.fit(x_train,y_train,
                                 epochs=20,
                                 batch_size=512,
                                 validation_data=(x_test,y_test))
smaller_model.compile(optimizer='rmsprop',
                       loss='binary_crossentropy',
                       metrics=['acc'])
					   
original과 smaller model의 val loss를 가져와서 float

import matplotlib.pyplot as plt

plt.plot(epochs, orginal_val_loss, 'b+', label='Original model')
plt.plot(epochs, smaller_model_val_loss, 'b+', label='Smaller model')
plt.xlabel('Epochs')
plt.ylabel('Validation loss')
plt.legend()

plt.show()

Listing 4.5 Version of the model with higher capacity

model=model.Sequential()
model.add(layers.Dense(512,activation='relu',input_shape=(10000,)))
model.add(layers.Dense(512,activation='relu'))
model.add(layers.Dense(1,activation='sigmoid'))

bigger_model.compile(optimizer='rmsprop',
                       loss='binary_crossentropy',
                       metrics=['acc'])
					   
bigger network train
smaller_hist=smaller_model.fit(x_train,y_train,
                                 epochs=20,
                                 batch_size=512,
                                 validation_data=(x_test,y_test))	
								 
bigger_model_val_loss=bigger_model_hist.history['val_loss']

plt.plot(epochs, orginal_val_loss, 'b+', label='Original model')
plt.plot(epochs, bigger_model_val_loss, 'b+', label='Bigger model')
plt.xlabel('Epochs')
plt.ylabel('Validation loss')
plt.legend()

plt.show()								

training loss에 대해 봐보자
original_train_loss=original_hist.history['loss']
bigger_model_train_loss=bigger_model_hist.history['loss']

plt.plot(epochs, original_train_loss, 'b+', label='Original model')
plt.plot(epochs, bigger_model_train_loss, 'b+', label='Bigger model')
plt.xlabel('Epochs')
plt.ylabel('Validation loss')
plt.legend()

plt.show()

Listing 4.6 Adding L2 weight regularization to the model

from tensorflow.keras import regularizers 

l2_model=models.Sequential()
l2_model.add(layers.Dense(16, kernel_regularizer=regularizer.l2(0.001), activation='relu',input_shape=(10000,)))
l2_model.add(layers.Dense(16, kernel_regularizer=regularizer.l2(0.001), activation='relu'))
l2_model.add(layers.Dense(1,activation='sigmoid'))

l2_model.compile(optimizer='rmsprop',
                       loss='binary_crossentropy',
                       metrics=['acc'])
					   
ls_model_val_loss=l2_model_hist.history['val_loss']

plt.plot(epochs, orginal_val_loss, 'b+', label='Original model')
plt.plot(epochs, l2_model_val_loss, 'b+', label='L2-regularized model')
plt.xlabel('Epochs')
plt.ylabel('Validation loss')
plt.legend()

plt.show()					

Listing 4.7 Different weight regularizers available in Keras

from tensorflow.keras import regularizers

regularizers.l1(0.001)
regularizers.l1_l2(l1=0.001. l2=0.001)

Listing 4.8 Adding dropout to the IMDB network

dpt_model=model.Sequential()
dpt_model.add(layers.Dense(16,activation='relu',input_shape=(10000,)))
dpt_model.add(layers.Dropout(0.5)) #50%를 dropout하겠다.
dpt_model.add(layers.Dense(16,activation='relu'))
dpt_model.add(layers.Dropout(0.5))
dpt_model.add(layers.Dense(1,activation='sigmoid'))

dpt_model.compile(optimizer='rmsprop',
                       loss='binary_crossentropy',
                       metrics=['acc'])
					   
dpt_hist=dpt_model.fit(x_train,y_train,
                       epochs=20,
                       batch_size=512,
                       validation_data=(x_test,y_test))					   

dpt_model_val_loss=dpt_model_hist.history['val_loss']

plt.plot(epochs, orginal_val_loss, 'b+', label='Original model')
plt.plot(epochs, dpt_model_val_loss, 'b+', label='Dropout-regularized model')
plt.xlabel('Epochs')
plt.ylabel('Validation loss')
plt.legend()

plt.show()

#At testing time
 layer_output *=0.5
#Or, at training time
 layer_output *=np.random.randint(0,high=2.size=layer_output.shape)
 layer_output /=0.5

Ch5. Deep Learning for Computer Vision

Listing 5.1 Instantiating a small convnet

from tensorflow.keras import layers
from tensorflow.keras import models

model=model.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(28, 28, 1))) 
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))

model.summary()

Listing 5.2 Adding a classifier on top of the convnet

model.add(layers.Flatten())
model.add(layers.Dense(64,activation='relu'))
model.add(layers.Dense(10,activation='softmax'))

model.summary()

from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_Images=train_images.reshape((60000, 28, 28, 1))
train_Images=train_images.astype('float32')/255

test_Images=train_images.reshape((10000, 28, 28, 1))
test_Images=train_images.astype('float32')/255

train_labels=to_categorical(train_labels)
test_labels=to_categorical(test_labels)

model.compile(optimizer='rmsprop',
             loss='categorical_crossentropy',
             metrics['accuracy'])
model.fit(train_images, train_labels, epochs=5, batch_size=64)

test_loss, test_acc=model.evaluate(test_images,test_labels)

test_acc

Listing 5.5 Instantiating a small convnet for dogs vs. cats classification

from keras import layers
from keras import models

model=model.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 3))) 
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))

model.add(layers.Flatten())
model.add(layers.Dense(512,activation='relu'))
model.add(layers.Dense(1,activation='sigmoid'))

Listing 5.6 Configuring the model for training

from keras import optimizers

model.compile(loss='categorical_crossentropy',
              optimizer=optimizers.RMSprop(lr=1e-4),
              metrics['acc'])
