Hold out validation

import numpy as np

num_validation_samples=10000

np.random.shuffle(data)

validation_data=data[:num_validation_samples]
data=data[num_validation_samples:]

training_data=data[:]

model=get_model()
model.train(training_data)
validation_score=model.evaluate(validation_data)

model=get_model()
model.train(np.concatenate([training_data,validation_data]))
test.score=model_evaluate(test_data)

K-fold cross-validation
k=4 #몇개의 fold
num_validation_samples=len(data) // k #validation sample에 몇개를 쓴지
np.random.shuffle(data) #data random shuffling
valdation_scores=[] #validation score를 4번에 걸쳐 구하면서 여기에 지정
for fold in range(k): #for문으로 fold를 4개를 돌리면서, training data는 validation set말고 다른 3개를 합쳐서 한다.
    validation_data=data[num_validation_samples * fold: num_validation_samples * (fold+1)]
    training_data=data[:num_validation_samples * fold] + data[num_validation_samples * (fold+1):]
    
    model=get_model()
    model.train(training_data)
    validation_score=model.evaluate(validation_data)
    validation_scores.append(validation_score)

validation_score=np.average(validation_scores) #최종 score는 validation_scores에 있는 값들을 평균내서 한다.

model=get_model()
model.train(data)
test_score=model.evaluate(test_data)

Reducing the network's size

Original model

from keras import models
from keras import layers

original_model=model.Sequential()
original_model.add(layers.Dense(16,activation='relu',input_shape=(10000,)))
original_model.add(layers.Dense(16,activation='relu'))
original_model.add(layers.Dense(1,activation='sigmoid'))

original_model.compile(optimizer='rmsprop',
                       loss='binary_crossentropy',
                       metrics=['acc'])

original network train

original_hist=original_model.fit(x_train,y_train,
                                 epochs=20,
                                 batch_size=512,
                                 validation_data=(x_test,y_test))
								 
Version of the model with lower capacity¶

smaller_model=model.Sequential()
smaller_model.add(layers.Dense(4,activation='relu',input_shape=(10000,)))
smaller_model.add(layers.Dense(4,activation='relu'))
smaller_model.add(layers.Dense(1,activation='sigmoid'))

smaller_model.compile(optimizer='rmsprop',
                       loss='binary_crossentropy',
                       metrics=['acc'])

smaller network train

smaller_hist=smaller_model.fit(x_train,y_train,
                                 epochs=20,
                                 batch_size=512,
                                 validation_data=(x_test,y_test))
smaller_model.compile(optimizer='rmsprop',
                       loss='binary_crossentropy',
                       metrics=['acc'])
					   
original과 smaller model의 val loss를 가져와서 float

import matplotlib.pyplot as plt

plt.plot(epochs, orginal_val_loss, 'b+', label='Original model')
plt.plot(epochs, smaller_model_val_loss, 'b+', label='Smaller model')
plt.xlabel('Epochs')
plt.ylabel('Validation loss')
plt.legend()

plt.show()

Listing 4.5 Version of the model with higher capacity

model=model.Sequential()
model.add(layers.Dense(512,activation='relu',input_shape=(10000,)))
model.add(layers.Dense(512,activation='relu'))
model.add(layers.Dense(1,activation='sigmoid'))

bigger_model.compile(optimizer='rmsprop',
                       loss='binary_crossentropy',
                       metrics=['acc'])
					   
bigger network train
smaller_hist=smaller_model.fit(x_train,y_train,
                                 epochs=20,
                                 batch_size=512,
                                 validation_data=(x_test,y_test))	
								 
bigger_model_val_loss=bigger_model_hist.history['val_loss']

plt.plot(epochs, orginal_val_loss, 'b+', label='Original model')
plt.plot(epochs, bigger_model_val_loss, 'b+', label='Bigger model')
plt.xlabel('Epochs')
plt.ylabel('Validation loss')
plt.legend()

plt.show()								

training loss에 대해 봐보자
original_train_loss=original_hist.history['loss']
bigger_model_train_loss=bigger_model_hist.history['loss']

plt.plot(epochs, original_train_loss, 'b+', label='Original model')
plt.plot(epochs, bigger_model_train_loss, 'b+', label='Bigger model')
plt.xlabel('Epochs')
plt.ylabel('Validation loss')
plt.legend()

plt.show()

Listing 4.6 Adding L2 weight regularization to the model

from tensorflow.keras import regularizers 

l2_model=models.Sequential()
l2_model.add(layers.Dense(16, kernel_regularizer=regularizer.l2(0.001), activation='relu',input_shape=(10000,)))
l2_model.add(layers.Dense(16, kernel_regularizer=regularizer.l2(0.001), activation='relu'))
l2_model.add(layers.Dense(1,activation='sigmoid'))

l2_model.compile(optimizer='rmsprop',
                       loss='binary_crossentropy',
                       metrics=['acc'])
					   
ls_model_val_loss=l2_model_hist.history['val_loss']

plt.plot(epochs, orginal_val_loss, 'b+', label='Original model')
plt.plot(epochs, l2_model_val_loss, 'b+', label='L2-regularized model')
plt.xlabel('Epochs')
plt.ylabel('Validation loss')
plt.legend()

plt.show()					

Listing 4.7 Different weight regularizers available in Keras

from tensorflow.keras import regularizers

regularizers.l1(0.001)
regularizers.l1_l2(l1=0.001. l2=0.001)

Listing 4.8 Adding dropout to the IMDB network

dpt_model=model.Sequential()
dpt_model.add(layers.Dense(16,activation='relu',input_shape=(10000,)))
dpt_model.add(layers.Dropout(0.5)) #50%를 dropout하겠다.
dpt_model.add(layers.Dense(16,activation='relu'))
dpt_model.add(layers.Dropout(0.5))
dpt_model.add(layers.Dense(1,activation='sigmoid'))

dpt_model.compile(optimizer='rmsprop',
                       loss='binary_crossentropy',
                       metrics=['acc'])
					   
dpt_hist=dpt_model.fit(x_train,y_train,
                       epochs=20,
                       batch_size=512,
                       validation_data=(x_test,y_test))					   

dpt_model_val_loss=dpt_model_hist.history['val_loss']

plt.plot(epochs, orginal_val_loss, 'b+', label='Original model')
plt.plot(epochs, dpt_model_val_loss, 'b+', label='Dropout-regularized model')
plt.xlabel('Epochs')
plt.ylabel('Validation loss')
plt.legend()

plt.show()

#At testing time
 layer_output *=0.5
#Or, at training time
 layer_output *=np.random.randint(0,high=2.size=layer_output.shape)
 layer_output /=0.5

Ch5. Deep Learning for Computer Vision

Listing 5.1 Instantiating a small convnet

from tensorflow.keras import layers
from tensorflow.keras import models

model=model.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(28, 28, 1))) 
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))

model.summary()

Listing 5.2 Adding a classifier on top of the convnet

model.add(layers.Flatten())
model.add(layers.Dense(64,activation='relu'))
model.add(layers.Dense(10,activation='softmax'))

model.summary()

from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical

(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_Images=train_images.reshape((60000, 28, 28, 1))
train_Images=train_images.astype('float32')/255

test_Images=train_images.reshape((10000, 28, 28, 1))
test_Images=train_images.astype('float32')/255

train_labels=to_categorical(train_labels)
test_labels=to_categorical(test_labels)

model.compile(optimizer='rmsprop',
             loss='categorical_crossentropy',
             metrics['accuracy'])
model.fit(train_images, train_labels, epochs=5, batch_size=64)

test_loss, test_acc=model.evaluate(test_images,test_labels)

test_acc

Listing 5.5 Instantiating a small convnet for dogs vs. cats classification

from keras import layers
from keras import models

model=model.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 3))) 
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))

model.add(layers.Flatten())
model.add(layers.Dense(512,activation='relu'))
model.add(layers.Dense(1,activation='sigmoid'))

Listing 5.6 Configuring the model for training

from keras import optimizers

model.compile(loss='categorical_crossentropy',
              optimizer=optimizers.RMSprop(lr=1e-4),
              metrics['acc'])

Listing 5.7 Using ImageDataGenerator to read images from directories

from keras.preprocessing.image import ImageDataGenerator

train_datagen=ImageDataGenerator(rescale=1./255)
test_datagen=ImageDataGenerator(rescale=1./255)

train_generator=train_datagen.flow_from_directory(
        train_dir,
        target_size=(150,150)
        batch_size=20.
        class_mode='binary')

validation_generator=test_datagen.flow_from_directory(
        validation_dir,
        target_size=(150,150)
        batch_size=20.
        class_mode='binary')
		
for data_batch, labels_batch in train_generator:
    print('data batch shape:', data_batch.shape)
    print('labels batch shape:', labels_batch.shape)
    break
	
history=model.fit_generator(
      train_genertor,
      steps_per_epoch=100,
      epochs=30
      validation_data=validation_generator,
      valdation_steps=50)
	  
model.save('cats_and_dogs_small_1.h5')

import matplotlib.pyplot as plt

acc=history.history['acc']
val_acc=history.history['val_acc']
loss=history.history['loss']
val_loss=history.history['val_loss']

epochs=range(len(acc))

plt.plot(epochs,acc,'bo',label='Training acc')
plt.plot(epochs,val_acc,'b',label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss,'bo',label='Training loss')
plt.plot(epochs, val_loss,'b',label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

Listing 5.11 Setting up a data augmentation configuration via ImageDataGenerator

datagen=ImageDataGenerator(
     rotation_range=40,
     width_shift_range=0.2,
     height_shift_range=0.2,
     shear_range=0.2,
     zoom_range=0.2,
     horizontal_flip=True,
     fill_mode='nearest')
	 
Listing 5.12 Displaying some randomly augmented training images

from keras.preprocessing import image

fnames=[os.path.join(train_cats_dir, fname) for fnamee in os.listdir(train_cats_dir)]

img_path=fnames[3]

img=image.load_img(img_path, target_size(150,150))

x=image.img_to_array(img)

x=x.reshape((1,)+x.shape)

i=0
for batch in datagen.flow(x, batch_size=1):
        plt.figure(i)
        imgplot=plt.imshow(image.array_to_img(batch[0]))
        i +=1
        if i%4==0:
            break
			
Listing 5.13 Defining a new convnet that includes dropout

model=model.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 3))) 
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2,2)))

model.add(layers.Flatten())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(512,activation='relu'))
model.add(layers.Dense(1,activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(lr=1e-4),
              metrics['acc'])
			  
Listing 5.14 Training the convnet using data-augmentation generators

train_datagen=ImageDataGenerator(
     resclae=1./255,
     rotation_range=40, #그림을 몇 도 돌릴건지.
     width_shift_range=0.2,
     height_shift_range=0.2,
     shear_range=0.2,
     zoom_range=0.2,
     horizontal_flip=True,)

test_datagen=ImageDataGenerator(rescale=1./255)

train_generator=train_datagen.flow_from_directory(
        train_dir,
        target_size=(150, 150),
        batch_size=32,
        class_mode='binary')
        
validation_generator=test_datagen.flow_from_directory(
        validation_dir,
        target_size=(150, 150),
        batch_size=32,
        class_mode='binary')

history=model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=100,
    validation_data=validation_generator,
    validation_steps=50)
	
Listing 5.16 Instantiating the VGG16 convolutional base

from kereas.application import VGG16

conv_base=VGG16(weights='imagenet',
                include_top=False,
                input_shape=(150, 150, 3))
				
Listing 5.17 Extracting feature using the pretrained convolutional base

import os
import numpy as np
from keras.preprocessing.image import ImageDataGenerator

base_dir='/Users/fchollet/Downloads/cats_and_domg_small'
train_dir=os.path.join(base_dir,'train')
validation_dir=os.path.join(base_dir,'validation')
test_dir=os.path.join(base_dir,'test')

datagen=ImageDataGenerator(rescale=1./255)
batch_size=20

def extract_features(directory. sample_count):
    features=np.zeros(shape=(sample_count, 4, 4, 512))
    labels=np.zeros(shape=(sample_count))
    generator=datagen.flow_from_directory(
        directory,
        target_size=(150,150),
        batch_size=batch_size,
        class_mode='binary')
    i=0
    for inputs_batch, labels_batch in generator:
        features_batch=conv_base.predict(inputs_batch)
        features[i * batch_size : (i+1) * batch_size]=features_batch
        labels[i * batch_size : (i+1) * batch_size]=labels_batch
        i += 1
        if i * batch_size >= sample_count:
            break
    return features, labels

train_features, train_labels=extract_features(train_dir,2000)
validation_features, validation_labels=extract_features(validation_dir,1000)
test_features, test_labels=extract_features(test_dir,1000)

Listing 5.18 Defining and training the densely connected classifier

from keras import layers
from keras import models
from keras import optimizers

model=model.Sequential()
model.add(layers.Dense(256, activation='relu',input_shape=4 * 4 * 512)  
model.add(layers.Dropout(0.5))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer=optimizers.RMSprop(lr=2e-5),
              loss='binary_crossentropy',
              metrics['acc'])
			  
history=model.fit(train_features, train_labels,
      epochs=30,
      batch_size=20,
      validation_data=(validation_features,validation_labels))
	  
Listing 5.19 Plotting the result

import matplotlib.pyplot as plt

acc=history.history['acc']
val_acc=history.history['val_acc']
loss=history.history['loss']
val_loss=history.history['val_loss']

epochs=range(1, len(acc)+1)

plt.plot(epochs,acc,'bo',label='Training acc')
plt.plot(epochs,val_acc,'b',label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss,'bo',label='Training loss')
plt.plot(epochs, val_loss,'b',label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

Listing 5.20 Adding a densely connected classifier on top of the convolutional base

from keras import models
from keras import layers

model=model.Sequential()
model.add(conv_base)
model.add(layers.Flatten())
model.add(layers.Dense(256, activation='relu'))  
model.add(layers.Dense(1, activation='sigmoid'))

Listing 5.21 Training the model end to end with a frozen convolutional base

from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers

train_datagen=ImageDataGenerator(
     resclae=1./255,
     rotation_range=40, 
     width_shift_range=0.2,
     height_shift_range=0.2,
     shear_range=0.2,
     zoom_range=0.2,
     horizontal_flip=True,
     fill_mode='nearest')

test_datagen=ImageDataGenerator(rescale=1./255)

train_generator=train_datagen.flow_from_directory(
        train_dir,
        target_size=(150, 150),
        batch_size=20,
        class_mode='binary')

validation_generator=test_datagen.flow_from_directory(
        validation_dir,
        target_size=(150, 150),
        batch_size=20,
        class_mode='binary')

model.compile(loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(lr=2e-5),
              metrics['acc'])

history=model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=30,
    validation_data=validation_generator,
    validation_steps=50)
